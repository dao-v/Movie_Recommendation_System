{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation System:\n",
    "\n",
    "This script was designed to predict the ratings of movies unseen by users according to the genres and tags associated with the movies they've liked and disliked. The dataset was obtained from grouplens (University of Minnesota, https://grouplens.org/datasets/movielens/), specifically the \"MovieLens 25M Dataset\": http://files.grouplens.org/datasets/movielens/ml-25m.zip (250 MB)<br>\n",
    "The resulting DataFrame produced by this script will choose the top ten movies with the highest predicted ratings.\n",
    "\n",
    "## Predictive Model Design:\n",
    "Predictions are a result of three models. The first model feeds in the genres from the movies that the like and disliked + the genres of the movie in question into a neural network/deep learning model and outputs a predicted rating. The second model is similar to the first model except it uses the tags associated with the movies that have been watched by the user and the tags associated with the movie in question to predict a rating. The third model takes the two predicted ratings from the first two models and predicts a final rating using linear regression.\n",
    "\n",
    "There is a potential that not all users will be included in this script because if a user had only watched one movie with little to no tags, it would not be possible to input them into the tags model.\n",
    "\n",
    "(Files: links.csv, genome-scores.csv, and genome-tags.csv are not used in this script)\n",
    "\n",
    "## How to Use:\n",
    "- To begin using this script, download the \"MovieLens 25M Dataset\" and extract the file (ml-23m) into the same directory where this script is located.\n",
    "- [Optional] Second, also unzip data.zip (from the GitHub repository) into the same directory as well.\n",
    "- Next, simply run all the cells in this notebook and all additional folders will be produced as needed. \n",
    "- Lastly, the top ten movies (as well as the predictions for all movies for a specific user) will be displayed in cells 20-22, using the userId of 6550 as an example. These two DataFrames will not be saved automatically and will only be there to show the results. \n",
    "\n",
    "## Things to Note:\n",
    "- All users are included in the statistics in cell 18, including those who've only rated a couple of movies. Most recommendation systems will attempt to address this by copying other users' profiles who've watched/rated the same movies over to the user with fewer data. This script does not directly deal with users with low data--instead, the movie(s) that the users did watch will have a majority (or sole) impact on what the models will predict.\n",
    "- Most of the cells will finish running within a couple of hours at most. However, cell 13 will most likely run for over 2 days before finishing. A copy of the resulting DataFrame is included in the GitHub repository in the correct location for the script to find the CSV file. Unless the original dataset is different from \"MovieLens 25M Dataset\" or just wanting to fully run the entire script, removal of the triple quotes in cell 13 is needed before running.\n",
    "- The very last cell (23) can be run to produce predictions for all users. However, the removal of the triple single quotes (comment syntax) is required before running.\n",
    "\n",
    "\n",
    "## Minimum System Requirements & Runtimes (IMPORTANT):\n",
    "Since the \"ratings.csv\" file is extremely large (25+ million rows), this script requires a large amount of RAM if using a personal system. My system has 48 GB of RAM and it is fully utilized during the model training for random forest (cell 18).  If you do not have that much RAM and (understandably) will not upgrade/add more RAM to your system, it is advised to splice a smaller subset of \"ratings.csv\" in cell 2 to a size that is more manageable to your system. In addition, setting a maximum length for each random forest tree and lower the number of trees trained in cell 18 might be required. From my experience, selecting a subset of \"ratings.csv\" for training does not seem to have a large impact on the performance of the models. If possible, loading in \"ratings.csv\" and shuffling it before subsetting is recommended since the CSV file is ordered by users.\n",
    "\n",
    "The entire script (without cells 13 and 23) should finish running within a day if the system is up-to-date on hardware and no major background applications are running. This script was ran using the GPU version of TensorFlow. If you do not have TensorFlow installed + have a supported Nvidia GPU in your system + are using an Anaconda distribution, install TensorFlow GPU using the instructions by Anaconda: https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\n",
    "\n",
    "The runtime for the last cell (cell 23) is expected to take many days.\n",
    "\n",
    "All components created and the original dataset will take up about 2.13 GB of storage. Running cell 23 is estimated to additionally take about 150 MB of storage.\n",
    "\n",
    "\n",
    "(No Python parallel programming is used in this script. Full CPU utilization is only used by Sklearn during the random forest training.)\n",
    "\n",
    "To compare, my system specifications are: <br>\n",
    "CPU = Intel i5-9600k (6-cores, factory settings) <br>\n",
    "GPU = Nvidia RTX 2070<br>\n",
    "Primary Storage Device = NVMe, Western Digital Black SN750<br>\n",
    "RAM Speed = DDR4 3000\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "VERSION 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from spellchecker import SpellChecker # pyspellchecker\n",
    "\n",
    "import re, os, math, sklearn, datetime, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2\n",
    "\n",
    "# Loading in all relevant datasets (ignoring links.csv, genome-scores.csv, genome-tags.csv)\n",
    "## Datasets are from: https://grouplens.org/datasets/movielens/25m/\n",
    "### Datasets are stored in the original folder name, \"ml-25m\"\n",
    "\n",
    "movies_df = pd.read_csv('ml-25m\\\\movies.csv')\n",
    "ratings_df = pd.read_csv('ml-25m\\\\ratings.csv')\n",
    "tags_df = pd.read_csv('ml-25m\\\\tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3\n",
    "\n",
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4\n",
    "\n",
    "ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5\n",
    "\n",
    "tags_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6\n",
    "\n",
    "# Moving the data out of ratings_df and tags_df for the last movie the user liked to be used as the label:\n",
    "## Rating of 4+ = liked\n",
    "# Creating directories:\n",
    "if os.path.exists('data\\\\') != True:\n",
    "    os.mkdir('data\\\\')\n",
    "    \n",
    "if os.path.exists('data\\\\last_liked_tags\\\\') != True: \n",
    "    os.mkdir('data\\\\last_liked_tags\\\\')\n",
    "    \n",
    "# Getting the last movie liked from ratings_df:\n",
    "ratings_df_copy = ratings_df.copy()\n",
    "tags_df_copy = tags_df.copy()\n",
    "\n",
    "users_list = list(set(ratings_df_copy.userId)) ## List of all users in the dataset\n",
    "\n",
    "ratings_index_list = [] ## These empty lists will be used to remove the last liked movies from the ratings_df and tags_df_mod copies\n",
    "tags_index_list = []\n",
    "\n",
    "last_ratings_df = pd.DataFrame() ## Want to save all the last liked movies rated into a single CSV file\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for user in users_list:\n",
    "    try: ## Some users did not rate a movie highly enough and will be removed from the dataset\n",
    "        temp_df = ratings_df_copy[ratings_df_copy.userId == user].copy()\n",
    "        temp_df = temp_df[temp_df.rating >= 4] # = Liked Movie\n",
    "\n",
    "        last_time = max(temp_df.timestamp) ## If the user did not have a \"liked\" movie, this will return an error\n",
    "\n",
    "        temp_df = temp_df[temp_df.timestamp == last_time] ## Isolating the last liked movie rated for each user\n",
    "        \n",
    "        if len(temp_df) > 1: ## Some of the movies were rated at the same timestamp; only the last one on spliced DF will be removed\n",
    "            temp_df = temp_df.iloc[[len(temp_df)-1]]\n",
    "            \n",
    "        ratings_index_list.append(temp_df.index.values[0]) ## Appending the index of the last movies watched\n",
    "\n",
    "        if counter == 0:\n",
    "            last_ratings_df = temp_df\n",
    "            counter = 1\n",
    "\n",
    "        else:\n",
    "            last_ratings_df = pd.concat([last_ratings_df, temp_df], ignore_index= True)\n",
    "        \n",
    "    except Exception:\n",
    "        ratings_index_list.append(ratings_df_copy[ratings_df_copy.userId == user].index.values[0]) ## Adding the index of the users whom did not highly rate a movie\n",
    "    \n",
    "    try:  ## Some users have not created tags\n",
    "        temp_df = tags_df_copy[tags_df_copy.userId == user].copy()\n",
    "        temp_df = temp_df[temp_df.rating >= 4]\n",
    "        last_movie = temp_df.movieId.values[0]\n",
    "        temp_df = temp_df[temp_df.movieId == last_movie]\n",
    "        \n",
    "        \n",
    "        if len(temp_df) == 0: ## MOST USERS DID NOT CREATE TAG(S) FOR THE LAST MOVIE LIKED\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            temp_df.to_csv('data\\\\last_liked_tags\\\\' + str(user) + '.csv')  ###!!! THESE TAGS WILL NOT BE USED AND IS STORED FOR EXAMINATION PURPOSES; these must be removed for \"proper\" datasets when training to exclude data related to the label from being used in the training data\n",
    "            tags_index_list.extend(list(temp_df.index.values))  ## This is a .extend since there are most likely more than one timestamp per movie\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "last_ratings_df.to_csv('data\\\\last_liked_ratings.csv')\n",
    "\n",
    "# Removing the last movies from ratings_df_copy and tags_df_copy:\n",
    "ratings_df_removed = ratings_df_copy.drop(ratings_index_list)\n",
    "\n",
    "tags_df_removed = tags_df_copy.drop(tags_index_list)\n",
    "\n",
    "ratings_df_removed.to_csv('data\\\\ratings_df_last_liked_movie_removed.csv')\n",
    "tags_df_removed.to_csv('data\\\\tags_df_last_liked_movie_removed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7\n",
    "\n",
    "'''\n",
    "\ttags.csv (remove timestamp column)\n",
    "\t\t- Lower case all tags\n",
    "\t\t- Remove tags with 1-2 letter words and remove parenthesis from the tags\n",
    "\t\t\tThese are opinions/more like mini-reviews\n",
    "\t\t\tIt will be hard to compare these into same catagories \n",
    "\t\t\tKEEP ALL TAGS WTIH \"based\"\n",
    "\t\t- Spell check all tags (use pyspellchecker: https://pypi.org/project/pyspellchecker/)  \n",
    "\t\t- Create a new DF\n",
    "\t\t\tAssociate each movie to the tags (ignoring/removing the userId)\n",
    "\t\t\t\tThen add all same tags together\n",
    "\t\t- Create a new DF\n",
    "\t\t\tAssociate each userId with all the tags they inputted\n",
    "\t\t\t\tCount all the tags\n",
    "\t\t\t\t\t\tThinking is that the tag with the most counts will be the subject/genre/type that the user likes to watch the most\n",
    "\t\t\t\t\t\t(Goal for this DF is to describe the user)\n",
    "'''\n",
    "\n",
    "## Deleting 'timestamp' column since it is not informative (not examining viewer's social behaviors)\n",
    "## Droping all NaN values, which is only seen in the tag column--this is to eliminate considering NaNs when looping below\n",
    "tags_df_removed = pd.read_csv('data\\\\tags_df_last_liked_movie_removed.csv', index_col= 0)\n",
    "\n",
    "tags_df_mod = tags_df_removed.copy().drop('timestamp', axis=1).dropna()\n",
    "tags_df_mod['tag'] = tags_df_mod['tag'].str.lower()# Making all tags lowercased for uniform format\n",
    "\n",
    "\n",
    "for index, row in tags_df_mod.iterrows():\n",
    "    tag = row.tag#.split()  ## splitting words for spell check\n",
    "    \n",
    "    correct_tag = re.sub(r' \\([^)]*\\)', '', tag)  ## Removing all parenthesis and its contents, including the whitespace before\n",
    "        \n",
    "    # First if:\n",
    "    if 'based' in correct_tag: ## This is necessary because it is a common tag and avoids the other if statements downstream\n",
    "        tags_df_mod.loc[index, 'tag'] = correct_tag\n",
    "        continue\n",
    "        \n",
    "    # Second if:    \n",
    "    if '-' in correct_tag: ## This is to keep \"sci-fi\" from being removed in the next if statement\n",
    "        tags_df_mod.loc[index, 'tag'] = correct_tag\n",
    "        continue\n",
    "        \n",
    "    # Third if:    \n",
    "    if re.findall(r'\\b\\w{2}\\b', correct_tag):\n",
    "        tags_df_mod.loc[index, 'tag'] = np.NaN ## Replacing two-letter words; Need to maintain index ordering, will delete NaNs later\n",
    "        \n",
    "    elif re.findall(r'\\b\\w{1}\\b', correct_tag):\n",
    "        tags_df_mod.loc[index, 'tag'] = np.NaN ## Replacing one-letter words\n",
    "        \n",
    "    elif tag == correct_tag: ## This is for better performance since replacing significantly slows the process\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        tags_df_mod.loc[index, 'tag'] = correct_tag ## Saves the corrected tag\n",
    "        pass\n",
    "        \n",
    "tags_df_mod = tags_df_mod.dropna() # Dropping all tags with words that are lower than two letters or less\n",
    "\n",
    "tags_df_mod.to_csv('data\\\\tags_df_mod.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8\n",
    "\n",
    "# Creating a new DF that contains the most common tags for each movie (\"movieId\"):\n",
    "\n",
    "## This will create a new DF for each movie and will store this file since there is no easy storage method for this task\n",
    "### Storage will be in the \"data\" folder under the \"movie_tags\" subfolder:\n",
    "    \n",
    "if os.path.exists('data\\\\movie_tags\\\\') != True: # Creating movie_tags subfolder\n",
    "    os.mkdir('data\\\\movie_tags\\\\')\n",
    "    \n",
    "## Creating a copy of tags_df_mod and dropping userID:\n",
    "tags_df_mod = pd.read_csv('data\\\\tags_df_mod.csv', index_col= 0)\n",
    "\n",
    "tags_df_no_user = tags_df_mod.copy().drop('userId', axis= 1)\n",
    "\n",
    "## Obtaining a list of all movieId with tags:\n",
    "### !!!! The set() function does not put the list in perfect order. Some of the IDs are out-of-place.\n",
    "movieId_list = list(set(tags_df_no_user.movieId))  \n",
    "\n",
    "for movieId in movieId_list:\n",
    "    df_select = tags_df_no_user[tags_df_no_user.movieId == movieId].copy().drop('movieId', axis= 1)\n",
    "    \n",
    "    df_select['COUNT'] = 1\n",
    "    \n",
    "    df_select_group = df_select.groupby(['tag']).count()\n",
    "    \n",
    "    df_select_group = df_select_group.sort_values(by=['COUNT'], ascending= False).reset_index()\n",
    "    \n",
    "    df_select_group.to_csv('data\\\\movie_tags\\\\' + str(movieId) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9\n",
    "\n",
    "# Creating a new DF that contains the most common tags for each user (\"userId\"):\n",
    "## This DF is similar to the movieId DF that is previously created except this ties the tags in with each user\n",
    "### This can be used in conjunction with the most common genres watched by the user to help determine which movies they like to watch\n",
    "if os.path.exists('data\\\\user_tags\\\\') != True: # Creating movie_tags subfolder\n",
    "    os.mkdir('data\\\\user_tags\\\\')\n",
    "    \n",
    "## Creating a copy of tags_df_mod and dropping userID:\n",
    "tags_df_mod = pd.read_csv('data\\\\tags_df_mod.csv', index_col= 0)\n",
    "\n",
    "tags_df_user = tags_df_mod.copy().drop('movieId', axis= 1)\n",
    "\n",
    "## Obtaining a list of all movieId with tags:\n",
    "userId_list = list(set(tags_df_user.userId))\n",
    "\n",
    "for userId in userId_list:\n",
    "    df_select = tags_df_user[tags_df_user.userId == userId].copy().drop('userId', axis= 1)\n",
    "    \n",
    "    df_select['COUNT'] = 1\n",
    "    \n",
    "    df_select_group = df_select.groupby(['tag']).count()\n",
    "    \n",
    "    df_select_group = df_select_group.sort_values(by=['COUNT'], ascending= False).reset_index()\n",
    "    \n",
    "    df_select_group.to_csv('data\\\\user_tags\\\\' + str(userId) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10\n",
    "\n",
    "# Creating another DF that contains the most common tags created by users:\n",
    "## Common = the tag was used 35 times or more\n",
    "tags_df_mod = pd.read_csv('data\\\\tags_df_mod.csv')\n",
    "\n",
    "common_tags_df = tags_df_mod.groupby(['tag']).count().sort_values('userId', ascending= False).copy().drop('movieId', axis= 1)\n",
    "\n",
    "common_tags_df = common_tags_df[common_tags_df.userId >= 35]\n",
    "\n",
    "common_tags_df.to_csv('data\\\\common_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11\n",
    "\n",
    "'''\n",
    "movies.csv\n",
    "\t\t- Move all years to its own Year column\n",
    "\t\t- Expand all genres into their own columns and use 0 & 1 as no or yes\n",
    "\t\t\tREMOVE \"IMAX\" from genre\n",
    "\t\t- Add average user rating from ratings.csv (include average + std & average - std)\n",
    "\t\t\tAlso add number of users who watched the movie\n",
    "'''\n",
    "ratings_df_removed = pd.read_csv('data\\\\ratings_df_last_liked_movie_removed.csv', index_col= 0)\n",
    "movies_df_mod = movies_df.copy()\n",
    "\n",
    "movies_df_mod['YEAR'] = 0\n",
    "movies_df_mod['UPPER_STD'] = 0\n",
    "movies_df_mod['LOWER_STD'] = 0\n",
    "movies_df_mod['AVG_RATING'] = 0\n",
    "movies_df_mod['VIEW_COUNT'] = 0\n",
    "\n",
    "# Making the genres into columns:\n",
    "## First, need to obtain a list of all the genres in the dataset.\n",
    "#### !!!! Note: \"IMAX\" is not listed in the readme but is present in the dataset. \"Children's\" in the readme is \"Children\" in the dataset.\n",
    "genres_list = []\n",
    "for index, row in movies_df.iterrows():\n",
    "    try:\n",
    "        genres = row.genres.split('|')\n",
    "        genres_list.extend(genres)\n",
    "    except:\n",
    "        genres_list.append(row.genres)\n",
    "        \n",
    "genres_list = list(set(genres_list))\n",
    "genres_list.remove('IMAX')\n",
    "genres_list.remove('(no genres listed)') # Replace with 'None'\n",
    "genres_list.append('None')\n",
    "\n",
    "for genre in genres_list: # Creating new columns with names as genres\n",
    "    movies_df_mod[genre] = 0  # 0 = movie is not considered in that genre\n",
    "\n",
    "\n",
    "for index, row in movies_df_mod.iterrows():\n",
    "    movieId = row.movieId\n",
    "    title = row.title\n",
    "    \n",
    "    try:\n",
    "        genres = row.genres.split('|') ## Multiple genres for the movie is separated by '|' in the one string; converts to list\n",
    "    except Exception:\n",
    "        genres = list(row.genres) ## In the case that there is only one genre for the movie\n",
    "        \n",
    "        \n",
    "    #print(index)\n",
    "    \n",
    "    # Extracting the year from the title:\n",
    "    try: ## Some titles do not have the year--these will be removed downstream to remove the need to access the IMDB API (http://www.omdbapi.com/)\n",
    "        matcher = re.compile('\\(\\d{4}\\)')  ## Need to extract '(year)' from the title in case there is a year in the title\n",
    "        parenthesis_year = matcher.search(title).group(0)\n",
    "        matcher = re.compile('\\d{4}') ## Matching the year from the already matched '(year)'\n",
    "        year = matcher.search(parenthesis_year).group(0)\n",
    "\n",
    "        movies_df_mod.loc[index, 'YEAR'] = int(year)\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Merging info from ratings_df into movies_df\n",
    "    try:\n",
    "        ratings_df_select = ratings_df_removed[ratings_df_removed.movieId == movieId]  ## Gathering the reviews for the movies\n",
    "        std = np.std(ratings_df_select.rating)\n",
    "        average_rating = np.mean(ratings_df_select.rating)\n",
    "\n",
    "        upper_std = average_rating + std\n",
    "\n",
    "        if upper_std > 5:   # This is to prevent the upper range from passing the max rating value\n",
    "            upper_std = 5\n",
    "\n",
    "        lower_std = average_rating - std\n",
    "\n",
    "        if lower_std < 0.5:\n",
    "            lower_std = 0.5\n",
    "\n",
    "        view_count = len(ratings_df_select)\n",
    "\n",
    "        movies_df_mod.loc[index, 'UPPER_STD'] = upper_std\n",
    "        movies_df_mod.loc[index, 'LOWER_STD'] = lower_std\n",
    "        movies_df_mod.loc[index, 'AVG_RATING'] = average_rating\n",
    "        movies_df_mod.loc[index, 'VIEW_COUNT'] = view_count\n",
    "        \n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # Changing all columns that are labelled as genres to 1 if the movie is in that genre:\n",
    "    if 'IMAX' in genres:\n",
    "        genres.remove('IMAX')\n",
    "        \n",
    "    if '(no genres listed)' in genres:\n",
    "        genres.remove('(no genres listed)')\n",
    "        genres.append('None')\n",
    "        \n",
    "    for genre in genres:\n",
    "        movies_df_mod.loc[index, genre] = 1\n",
    "        \n",
    "movies_df_mod = movies_df_mod[movies_df_mod.YEAR != 0] ## Removing all movies without years in the title\n",
    "movies_df_mod = movies_df_mod[movies_df_mod.VIEW_COUNT != 0] ## Removing all movies than have not be rated\n",
    "\n",
    "movies_df_mod.to_csv('data\\\\movies_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12\n",
    "\n",
    "# Combining ratings_df and movies_df_mod together:\n",
    "movies_df_mod = pd.read_csv('data\\\\movies_mod.csv')\n",
    "\n",
    "ratings_df_removed = pd.read_csv('data\\\\ratings_df_last_liked_movie_removed.csv')\n",
    "\n",
    "ratings_movies_df = ratings_df_removed.merge(movies_df_mod, how= 'left', on= 'movieId').dropna()  ## Some of the movies were removed when creating movies_df_mod, which will result in nan values for some rows\n",
    "\n",
    "# Getting a count of all the liked and dislike genres and transforming it into a percentage (liked genre counts / all liked genres counts)\n",
    "## If the user rated the movie 4+, then they liked it. If lower than 4, then they disliked it.\n",
    "users_list = list(set(ratings_movies_df.userId))\n",
    "\n",
    "total_user_like_df = pd.DataFrame()\n",
    "total_user_dislike_df = pd.DataFrame()\n",
    "\n",
    "progress_counter_1 = 0\n",
    "progress_counter_2 = .10\n",
    "\n",
    "for user in users_list:\n",
    "    temp_df = ratings_movies_df[ratings_movies_df.userId == user]\n",
    "    like_df = temp_df[temp_df.rating >= 4].iloc[:, 14:] ## Only selecting the genres\n",
    "    dislike_df = temp_df[temp_df.rating < 4].iloc[:, 14:]\n",
    "    \n",
    "    liked_total_counts = 0\n",
    "    liked_dict = {'userId': user,'War': 0, 'Animation': 0, 'Horror': 0, 'Sci-Fi': 0, 'Fantasy': 0, 'Thriller': 0, 'Crime': 0, 'Mystery': 0, \n",
    "                  'Documentary': 0, 'Children': 0, 'Action': 0, 'Adventure': 0, 'Musical': 0,'Film-Noir': 0, 'Drama': 0, \n",
    "                  'Romance': 0, 'Comedy': 0, 'Western': 0, 'None': 0}\n",
    "    \n",
    "    disliked_total_counts = 0\n",
    "    disliked_dict = {'userId': user,'War': 0, 'Animation': 0, 'Horror': 0, 'Sci-Fi': 0, 'Fantasy': 0, 'Thriller': 0, 'Crime': 0, 'Mystery': 0, \n",
    "                  'Documentary': 0, 'Children': 0, 'Action': 0, 'Adventure': 0, 'Musical': 0,'Film-Noir': 0, 'Drama': 0, \n",
    "                  'Romance': 0, 'Comedy': 0, 'Western': 0, 'None': 0}   \n",
    "    \n",
    "    progress_counter_1 += 1\n",
    "    if progress_counter_1 / len(users_list) >= progress_counter_2:\n",
    "        print(progress_counter_1 / len(users_list) * 100, '%')\n",
    "        progress_counter_2 += .10\n",
    "    \n",
    "    for genre in list(like_df.columns): ## Getting all the genre counts for liked and disliked, separately\n",
    "        if len(like_df) == 0: ## If the user has not given a movie a rating of 4 or higher\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            liked_total_counts += sum(like_df[genre])\n",
    "        \n",
    "        \n",
    "        if len(dislike_df) == 0: ## If the user has not given a movie a rating of 3.5 or lower\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            disliked_total_counts += sum(dislike_df[genre])\n",
    "        \n",
    "        \n",
    "    for genre in list(like_df.columns):\n",
    "        if liked_total_counts == 0: \n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            liked_genre_total_counts = sum(like_df[genre])\n",
    "            liked_dict[genre] = liked_genre_total_counts/liked_total_counts\n",
    "            \n",
    "            \n",
    "        if disliked_total_counts == 0:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            disliked_genre_total_counts = sum(dislike_df[genre])\n",
    "            disliked_dict[genre] = disliked_genre_total_counts/disliked_total_counts\n",
    "        \n",
    "    \n",
    "    user_like_df = pd.DataFrame(liked_dict, index=[0]) ## Even though some users have not rated a movie higher or lower than 4, the zero counts will still be added for complete-ness\n",
    "    user_dislike_df = pd.DataFrame(disliked_dict, index=[0])\n",
    "    \n",
    "    # Concatenating the user total counts \n",
    "    if len(total_user_like_df) == 0:\n",
    "        total_user_like_df = user_like_df\n",
    "    \n",
    "    else:\n",
    "        total_user_like_df = pd.concat([total_user_like_df, user_like_df], ignore_index= True)\n",
    "        \n",
    "    if len(total_user_dislike_df) == 0:\n",
    "        total_user_dislike_df = user_dislike_df\n",
    "        \n",
    "    else:\n",
    "        total_user_dislike_df = pd.concat([total_user_dislike_df, user_dislike_df], ignore_index= True)\n",
    "        \n",
    "total_user_like_df.to_csv('data\\\\total_user_like_df.csv')\n",
    "total_user_dislike_df.to_csv('data\\\\total_user_dislike_df.csv')\n",
    "        \n",
    "##########################################\n",
    "# The reason why the counts are in percentage is so that the counts/genres are scaled against each other rather than a raw count\n",
    "## This is more important for the models since someone who rated a lot of movies vs someone who rated a few movies would have higher counts\n",
    "## but the higher counts is not meaningful and will most likely skew the model weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13\n",
    "\n",
    "# !!! This cell will most likely take over 2 days or more on a personal system. \n",
    "# !!! A premade CSV file is already included in the GitHub file (like_dislike_tags.csv) if not wanting to wait and the original dataset is the \"MovieLens 25M Dataset\".\n",
    "# !!! Else, remove the triple single quotes before running this cell.\n",
    "'''\n",
    "# Creating a dictionary of vectorized tags:\n",
    "if os.path.exists('data\\\\final\\\\') != True:\n",
    "    os.mkdir('data\\\\final\\\\')\n",
    "\n",
    "common_tags = pd.read_csv('data\\\\common_tags.csv')\n",
    "\n",
    "tags = list(set(common_tags.tag))\n",
    "\n",
    "vector_counter = 0\n",
    "vectorized_dict = {}\n",
    "\n",
    "for tag in tags:\n",
    "    vectorized_dict[tag] = vector_counter\n",
    "    vector_counter += 1\n",
    "    \n",
    "ratings_df_removed = pd.read_csv('data\\\\ratings_df_last_liked_movie_removed.csv', index_col= 0)\n",
    "\n",
    "user_list = list(set(ratings_df_removed.userId))\n",
    "\n",
    "like_dislike_tags = pd.DataFrame()\n",
    "index_counter = 0\n",
    "\n",
    "progress_counter_1 = 0\n",
    "progress_counter_2 = 5\n",
    "start_time = datetime.datetime.now()\n",
    "print('Start Time:', start_time)\n",
    "\n",
    "for user in user_list:\n",
    "    progress_counter_1 += 1\n",
    "\n",
    "    temp_ratings_df = ratings_df_removed[ratings_df_removed.userId == user]\n",
    "    like_tags_df = pd.DataFrame()\n",
    "    dislike_tags_df = pd.DataFrame()\n",
    "        \n",
    "    for index, row in temp_ratings_df.iterrows():  ## Creating tags for each user\n",
    "        try: ### This is to check if the movie tags exist\n",
    "            if row.rating >= 4: # Like\n",
    "                temp_movie_df = pd.read_csv('data\\\\movie_tags\\\\{}.csv'.format(str(int(row.movieId)))) ## This oddly turns the movieId into a float, most likely to match the other data types in the selected series\n",
    "\n",
    "                if len(like_tags_df) == 0:\n",
    "                    like_tags_df = temp_movie_df\n",
    "\n",
    "                else:\n",
    "                    like_tags_df = pd.concat([like_tags_df, temp_movie_df], ignore_index= True)\n",
    "\n",
    "            else:\n",
    "                temp_movie_df = pd.read_csv('data\\\\movie_tags\\\\{}.csv'.format(str(int(row.movieId))))\n",
    "\n",
    "                if len(like_tags_df) == 0:\n",
    "                    dislike_tags_df = temp_movie_df\n",
    "\n",
    "                else:\n",
    "                    dislike_tags_df = pd.concat([dislike_tags_df, temp_movie_df], ignore_index= True)\n",
    "        except Exception:\n",
    "            pass\n",
    "                \n",
    "    ## Counting all tags\n",
    "    try:  ### This is to check if the user has movies they've liked or disliked. Users who only have liked movies will be skipped (example: userId 173)\n",
    "        like_tags_list = list(like_tags_df.tag)\n",
    "        dislike_tags_list = list(dislike_tags_df.tag)\n",
    "    except Exception:\n",
    "        continue\n",
    "    \n",
    "    like_dict = {}\n",
    "    dislike_dict = {}\n",
    "    \n",
    "    for tag in like_tags_list:\n",
    "        like_dict[tag] = like_tags_list.count(tag) * -1  ### This is multiple by -1 to convert it to a negative numerical count for the sorting that will be done next\n",
    "    \n",
    "    for tag in dislike_tags_list:\n",
    "        dislike_dict[tag] = dislike_tags_list.count(tag) * -1\n",
    "        \n",
    "    ## Sorting the dictionary by the tag counts (smallest to largest is by default and simplest; in this case, the multiplication by -1 makes the tags with the largest counts the first in the sorted list)\n",
    "    like_tags_counted = sorted(like_dict, key= lambda tag: like_dict[tag])  ## Returns a list of the tags\n",
    "    dislike_tags_counted = sorted(dislike_dict, key= lambda tag: dislike_dict[tag])\n",
    "    \n",
    "    ## Converting the tags to vectorized tags but only for the first 50 tags from the like and dislike tags counted lists\n",
    "    like_tags_vectorized = []\n",
    "    dislike_tags_vectorized = []\n",
    "    \n",
    "    if len(like_tags_counted) < 50:  ## Checking to make sure there is 50 tags in the counted lists\n",
    "        num_like_tags = len(like_tags_counted)\n",
    "    else:\n",
    "        num_like_tags = 50\n",
    "        \n",
    "    if len(dislike_tags_counted) < 50: \n",
    "        num_dislike_tags = len(like_tags_counted)\n",
    "    else:\n",
    "        num_dislike_tags = 50\n",
    "    \n",
    "    for tag in like_tags_counted[:num_like_tags]:\n",
    "        try:  ### The tag might not exist in the vectorized dictionary\n",
    "            tag_vector = vectorized_dict[tag]\n",
    "            like_tags_vectorized.append(tag_vector)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "    for tag in dislike_tags_counted[:num_dislike_tags]:\n",
    "        try:\n",
    "            tag_vector = vectorized_dict[tag]\n",
    "            dislike_tags_vectorized.append(tag_vector)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "    if len(like_tags_vectorized) < 20 or len(dislike_tags_vectorized) < 20:\n",
    "        continue  ## If any of the two are not 20 tags in length, then the user will be skipped\n",
    "    \n",
    "    ## Obtaining the most liked and disliked tags, 20 tags each, and adding it to like_dislike_tags:\n",
    "    like_dislike_dict = {}\n",
    "    \n",
    "    like_dislike_dict['userId'] = user\n",
    "    \n",
    "    for x in range(20):\n",
    "        like_dislike_dict['LIKE_' + str(x)] = like_tags_vectorized[x]\n",
    "        like_dislike_dict['DISLIKE_' + str(x)] = dislike_tags_vectorized[x]\n",
    "    \n",
    "    concat_df = pd.DataFrame(like_dislike_dict, index=[0])\n",
    "    \n",
    "    if len(like_dislike_tags) == 0:\n",
    "        like_dislike_tags = concat_df\n",
    "    \n",
    "    else:\n",
    "        like_dislike_tags = pd.concat([like_dislike_tags, concat_df], ignore_index= True)\n",
    "    \n",
    "    if (progress_counter_1 / len(user_list)) * 100 >= progress_counter_2:\n",
    "        print((progress_counter_1 / len(user_list)) * 100, '% completed')\n",
    "        print('Processing Time:', datetime.datetime.now() - start_time)\n",
    "        print('Current Time:', datetime.datetime.now())\n",
    "        progress_counter_2 += 5\n",
    "\n",
    "like_dislike_tags = like_dislike_tags.astype('int64')\n",
    "like_dislike_tags.to_csv('data\\\\final\\\\like_dislike_tags.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14\n",
    "\n",
    "# Creating a movie tags profile to complement the user tags:\n",
    "if os.path.exists('data\\\\final\\\\') != True:\n",
    "    os.mkdir('data\\\\final\\\\')\n",
    "    \n",
    "movies_df_mod = pd.read_csv('data\\\\movies_mod.csv', index_col= 0)\n",
    "movieId_list = list(movies_df_mod.movieId)\n",
    "del movies_df_mod\n",
    "\n",
    "movie_tags_df = pd.DataFrame()\n",
    "index_counter = 0\n",
    "\n",
    "progress_counter_1 = 0\n",
    "progress_counter_2 = 5\n",
    "start_time = datetime.datetime.now()\n",
    "print('Start Time:', start_time)\n",
    "\n",
    "\n",
    "\n",
    "for movie in movieId_list:\n",
    "    progress_counter_1 += 1\n",
    "\n",
    "    try:\n",
    "        temp_df = pd.read_csv('data\\\\movie_tags\\\\{}.csv'.format(movie), index_col= 0)  ## The tags are already in order of most counts and then alphabetically\n",
    "\n",
    "        if len(temp_df) < 5: ## Skipping movies with less than 5 tags\n",
    "            continue \n",
    "\n",
    "        vectorized_tag = []\n",
    "        movie_tags = list(temp_df.tag)\n",
    "\n",
    "        for tag in movie_tags:\n",
    "            try:\n",
    "                tag_vector = vectorized_dict[tag]\n",
    "                vectorized_tag.append(tag_vector)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if len(vectorized_tag) < 5: ## Skipping movies with less than 5 common tags; The first similar if statement is not needed but is placed for performance purposes\n",
    "            continue \n",
    "\n",
    "        movie_tags_df.loc[index_counter, 'movieId'] = movie\n",
    "\n",
    "        for x in range(5):\n",
    "            movie_tags_df.loc[index_counter, 'TAG_' + str(x)] = vectorized_tag[x]\n",
    "            \n",
    "        index_counter += 1\n",
    "            \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if (progress_counter_1 / len(movieId_list)) * 100 >= progress_counter_2:\n",
    "        print((progress_counter_1 / len(movieId_list)) * 100, '% completed')\n",
    "        print('Processing Time:', datetime.datetime.now() - start_time)\n",
    "        print('Current Time:', datetime.datetime.now())\n",
    "        progress_counter_2 += 5\n",
    "\n",
    "movie_tags_df.to_csv('data\\\\final\\\\movie_tags_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 15\n",
    "\n",
    "def stats(predictions, true, flex_range= 0.5):\n",
    "    predictions_list = []\n",
    "    round_list = np.arange(0.5, 5.5, 0.5)\n",
    "\n",
    "    for value in predictions:\n",
    "        value_ori = value\n",
    "        compare_diff = 99999\n",
    "        value_round = 0\n",
    "\n",
    "        for rating in round_list:\n",
    "            compare_value = abs(value_ori - rating)\n",
    "\n",
    "            if compare_value < compare_diff: ## The absolute difference value that is closest to 0 is the rating the prediction will be rounded to\n",
    "                compare_diff = compare_value\n",
    "                value_round = rating\n",
    "\n",
    "        predictions_list.append(value_round)\n",
    "\n",
    "    prediction_dict = {'PREDICTION': predictions_list, 'TRUE': list(true)}\n",
    "    prediction_compare_df = pd.DataFrame(prediction_dict)\n",
    "\n",
    "    rating_accuracy = 0\n",
    "    like_dislike_tp = 0  ## \"Positive\" = Like\n",
    "    like_dislike_tn = 0  ## \"Negative\" = Dislike\n",
    "    like_dislike_fp = 0\n",
    "    like_dislike_fn = 0\n",
    "    prediction_length = len(prediction_compare_df)\n",
    "\n",
    "    ## Making the accuracy definition more flexible by covering a larger range:\n",
    "    rating_accuracy_flex = 0  ## If the prediction was within +/- 0.5 of the actual\n",
    "    like_dislike_tp_flex = 0  ## If the prediction was 3.5+ (instead of 4+), then it is a like\n",
    "    like_dislike_tn_flex = 0  ## If the prediction was 3.0-, then it is a dislike\n",
    "    like_dislike_fp_flex = 0\n",
    "    like_dislike_fn_flex = 0\n",
    "\n",
    "    progress_counter = 0\n",
    "\n",
    "    for index, row in prediction_compare_df.iterrows():\n",
    "        predict_like = 0\n",
    "        true_like = 0\n",
    "\n",
    "        if row.PREDICTION >= 4:\n",
    "            predict_like = 1\n",
    "\n",
    "        if row.TRUE >= 4:\n",
    "            true_like = 1\n",
    "\n",
    "        if row.PREDICTION == row.TRUE:  ## This is if the exact predicted rating value is the same as the actual value\n",
    "            rating_accuracy += 1\n",
    "\n",
    "        if predict_like == true_like:\n",
    "            if predict_like == 1:  ## Don't need to consider true_like to also be 1 since it is assumed it is with the nested if condition\n",
    "                like_dislike_tp += 1  ## True Positive\n",
    "\n",
    "            else:\n",
    "                like_dislike_tn += 1  ## True Negative\n",
    "\n",
    "        else:\n",
    "            if predict_like == 1:\n",
    "                like_dislike_fp += 1  ## False Positive\n",
    "\n",
    "            else:\n",
    "                like_dislike_fn += 1 ## False Negative\n",
    "\n",
    "        ####### FLEX starts:\n",
    "        predict_like_flex = 0\n",
    "        true_like_flex = 0\n",
    "\n",
    "        if row.PREDICTION >= 3.5:\n",
    "            predict_like_flex = 1\n",
    "\n",
    "        if row.TRUE >= 3.5:\n",
    "            true_like_flex = 1\n",
    "\n",
    "        if row.PREDICTION >= (row.TRUE - flex_range) and row.PREDICTION <= (row.TRUE + flex_range):  \n",
    "            rating_accuracy_flex += 1\n",
    "\n",
    "        if predict_like_flex == true_like_flex:\n",
    "            if predict_like_flex == 1:  \n",
    "                like_dislike_tp_flex += 1 \n",
    "\n",
    "            else:\n",
    "                like_dislike_tn_flex += 1 \n",
    "\n",
    "        else:\n",
    "            if predict_like_flex == 1:\n",
    "                like_dislike_fp_flex += 1 \n",
    "\n",
    "            else:\n",
    "                like_dislike_fn_flex += 1 \n",
    "\n",
    "        progress_counter += 1\n",
    "        if progress_counter % 100000 == 0:\n",
    "            print(str(progress_counter / prediction_length * 100) + '%')\n",
    "\n",
    "    rating_accuracy = rating_accuracy / prediction_length\n",
    "    like_dislike_accuracy = (like_dislike_tp + like_dislike_tn) / prediction_length\n",
    "\n",
    "    rating_accuracy_flex = rating_accuracy_flex / prediction_length\n",
    "    like_dislike_accuracy_flex = (like_dislike_tp_flex + like_dislike_tn_flex) / prediction_length\n",
    "\n",
    "    print('True Positive: {}, True Negative: {}, False Positive {}, False Negative {}'.format(like_dislike_tp, like_dislike_tn, like_dislike_fp, like_dislike_fn))\n",
    "    print('Rating Accuracy: {}, Catagorical Accuracy (Like/Dislike) {}'.format(rating_accuracy, like_dislike_accuracy))\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "    print('FLEX True Positive: {}, FLEX True Negative: {}, FLEX False Positive {}, FLEX False Negative {}'.format(like_dislike_tp_flex, like_dislike_tn_flex, like_dislike_fp_flex, like_dislike_fn_flex))\n",
    "    print('FLEX Rating Accuracy: {}, FLEX Catagorical Accuracy (Like/Dislike) {}'.format(rating_accuracy_flex, like_dislike_accuracy_flex))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 16\n",
    "\n",
    "def merge_shuffle_split(split=0.5):\n",
    "    movies_df_mod = pd.read_csv('data\\\\movies_mod.csv', index_col= 0)\n",
    "    ratings_df_removed = pd.read_csv('data\\\\ratings_df_last_liked_movie_removed.csv', index_col= 0)\n",
    "    \n",
    "    # Since ratings_df_removed is the template for merging, it will be shuffled:\n",
    "    ratings_df_removed = shuffle(ratings_df_removed)\n",
    "    \n",
    "    # Selecting a certain range from ratings_df_removed, train + test:\n",
    "    selection_range = int(len(ratings_df_removed) * (split))\n",
    "    ratings_df_removed = ratings_df_removed.iloc[: selection_range, :]\n",
    "    \n",
    "    # Merging begins:\n",
    "    ratings_df_removed = ratings_df_removed.merge(movies_df_mod, how= 'left', on= 'movieId').dropna()\n",
    "    del movies_df_mod\n",
    "\n",
    "\n",
    "    # Changing the columns names to differentiate between the columns of total_user_like_df and total_user_dislike_df:\n",
    "    total_user_like_df = pd.read_csv('data\\\\total_user_like_df.csv', index_col= 0)\n",
    "\n",
    "    like_columns = list(total_user_like_df.columns)\n",
    "    like_columns_modified = []\n",
    "\n",
    "    for column in like_columns:\n",
    "        if column == 'userId':\n",
    "            like_columns_modified.append('userId')\n",
    "        else:\n",
    "            modify_column = 'user_like_' + column\n",
    "            like_columns_modified.append(modify_column)\n",
    "\n",
    "    total_user_like_df.columns = like_columns_modified\n",
    "\n",
    "    ratings_df_removed = ratings_df_removed.merge(total_user_like_df, how= 'left', on= 'userId').dropna()\n",
    "    del total_user_like_df\n",
    "    \n",
    "\n",
    "    total_user_dislike_df = pd.read_csv('data\\\\total_user_dislike_df.csv', index_col= 0)    \n",
    "\n",
    "    dislike_columns = list(total_user_dislike_df.columns)\n",
    "    dislike_columns_modified = []\n",
    "\n",
    "    for column in dislike_columns:\n",
    "        if column == 'userId':\n",
    "            dislike_columns_modified.append('userId')\n",
    "        else:\n",
    "            modify_column = 'user_dislike_' + column\n",
    "            dislike_columns_modified.append(modify_column)\n",
    "\n",
    "    total_user_dislike_df.columns = dislike_columns_modified\n",
    "\n",
    "    # Merging all the DFs to create one final DF:\n",
    "    ratings_df_removed = ratings_df_removed.merge(total_user_dislike_df, how= 'left', on= 'userId').dropna()\n",
    "\n",
    "    # Removing loaded DFs to save on RAM space:\n",
    "    del total_user_dislike_df\n",
    "\n",
    "    movie_tags_df = pd.read_csv('data\\\\final\\\\movie_tags_df.csv', index_col= 0)\n",
    "    ratings_df_removed = ratings_df_removed.merge(movie_tags_df, how= 'left', on= 'movieId').dropna()\n",
    "    del movie_tags_df\n",
    "\n",
    "    like_dislike_tags = (pd.read_csv('data\\\\final\\\\like_dislike_tags.csv', index_col= 0)).astype('int64')\n",
    "    ratings_df_removed = ratings_df_removed.merge(like_dislike_tags, how= 'left', on= 'userId').dropna()\n",
    "    del like_dislike_tags\n",
    "    \n",
    "    like_columns_modified.remove('userId')\n",
    "    dislike_columns_modified.remove('userId')\n",
    "    like_columns.remove('userId')\n",
    "    \n",
    "    genres_like = ratings_df_removed.loc[:, like_columns_modified]\n",
    "    genres_dislike = ratings_df_removed.loc[:, dislike_columns_modified]\n",
    "    genres_movie = ratings_df_removed.loc[:, like_columns]\n",
    "    \n",
    "    # Generating the columns for the tag inputs for random forest:\n",
    "    rf_columns = []\n",
    "    for x in range(20): \n",
    "        rf_columns.append('LIKE_' + str(x))\n",
    "        rf_columns.append('DISLIKE_' + str(x))\n",
    "    for x in range(5):\n",
    "        rf_columns.append('TAG_' + str(x))\n",
    "        \n",
    "    rf_input = ratings_df_removed.loc[:, rf_columns]\n",
    "    \n",
    "    ratings = list(ratings_df_removed.rating)\n",
    "    \n",
    "    del ratings_df_removed\n",
    "    \n",
    "    return genres_like, genres_dislike, genres_movie, rf_input, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 17\n",
    "\n",
    "# Using Deep Learning/TensorFlow as the first model:\n",
    "## The goal of the model is to predict the rating the person would give to each movie\n",
    "### There will be three inputs: user liked genres, user disliked genres, and movie genres\n",
    "### The label will be the actual rating for the movie that the user gave it\n",
    "user_liked_genres = keras.Input(shape= (19,))\n",
    "user_disliked_genres = keras.Input(shape= (19,))\n",
    "movie_genres = keras.Input(shape= (19,))\n",
    "\n",
    "## Liked genres Input:\n",
    "liked_input = keras.layers.Dense(19, activation= 'relu')(user_liked_genres)\n",
    "liked_hidden_1 = keras.layers.Dense(50, activation= 'relu')(liked_input)\n",
    "liked_hidden_2 = keras.layers.Dense(50, activation= 'relu')(liked_hidden_1)\n",
    "\n",
    "## Disliked genres Input:\n",
    "disliked_input = keras.layers.Dense(19, activation= 'relu')(user_disliked_genres)\n",
    "disliked_hidden_1 = keras.layers.Dense(50, activation= 'relu')(disliked_input)\n",
    "disliked_hidden_2 = keras.layers.Dense(50, activation= 'relu')(disliked_hidden_1)\n",
    "\n",
    "## Movie genres Input:\n",
    "movie_input = keras.layers.Dense(19, activation= 'relu')(movie_genres)\n",
    "movie_hidden_1 = keras.layers.Dense(50, activation= 'relu')(movie_input)\n",
    "movie_hidden_2 = keras.layers.Dense(50, activation= 'relu')(movie_hidden_1)\n",
    "\n",
    "## Merging:\n",
    "merged_model = keras.layers.concatenate([liked_hidden_2, disliked_hidden_2, movie_hidden_2])\n",
    "merged_model_hidden_1 = keras.layers.Dense(150, activation= 'relu')(merged_model)\n",
    "merged_model_hidden_2 = keras.layers.Dense(75, activation= 'relu')(merged_model_hidden_1)\n",
    "merged_model_hidden_3 = keras.layers.Dense(50, activation= 'relu')(merged_model_hidden_2)\n",
    "\n",
    "## Output Layer:\n",
    "output_rating = keras.layers.Dense(1, activation= 'sigmoid')(merged_model_hidden_3)\n",
    "\n",
    "## Molding the Model togther:\n",
    "genres_model = keras.Model(inputs= [user_liked_genres, user_disliked_genres, movie_genres], outputs= output_rating)\n",
    "\n",
    "## Compiling the Model:\n",
    "genres_model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001), loss= 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# CELL 18\n",
    "\n",
    "# Models Training:\n",
    "if os.path.exists('models\\\\') != True: \n",
    "    os.mkdir('models\\\\')\n",
    "\n",
    "# Generating the datasets:\n",
    "genres_like, genres_dislike, genres_movie, rf_input, ratings = merge_shuffle_split() # Default split of the whole ratings.csv dataset is set to be 50%; already shuffled\n",
    "\n",
    "train_split = 0.5 ## This would be about 25% of original ratings.csv dataset\n",
    "split_index = int(len(ratings) * train_split)\n",
    "\n",
    "genres_like_train = genres_like.iloc[: split_index, :]\n",
    "genres_like_test = genres_like.iloc[split_index :, :]\n",
    "del genres_like ## Attempting to save RAM space\n",
    "\n",
    "genres_dislike_train = genres_dislike.iloc[: split_index, :]\n",
    "genres_dislike_test = genres_dislike.iloc[split_index :, :]\n",
    "del genres_dislike\n",
    "\n",
    "genres_movie_train = genres_movie.iloc[: split_index, :]\n",
    "genres_movie_test = genres_movie.iloc[split_index :, :]\n",
    "del genres_movie\n",
    "\n",
    "ratings_scaled = np.array(ratings) / 5\n",
    "ratings_scaled_train = ratings_scaled[: split_index]\n",
    "ratings_scaled_test = ratings_scaled[split_index :]\n",
    "\n",
    "batch_size = 500\n",
    "epochs = 10\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * math.exp(0.1 * (5 - epoch))\n",
    "\n",
    "Learning_Rate_Callback = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "class Save_Progress_Callback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None): ## Saving and printing after each epoch\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        print(\"Epoch {}, loss is {:7.3f}, validation loss is {:7.3f}, learning rate is {}.\".format(epoch, logs[\"loss\"], logs[\"val_loss\"], lr))\n",
    "            \n",
    "### !!!!!!!!!!!!!! VERBOSE MUST BE SET TO 0 AS THE OUTPUT IS TOO LONG/LARGE AND WILL CRASH THE NOTEBOOK    \n",
    "genres_model.fit(x= [genres_like_train, genres_dislike_train, genres_movie_train], \n",
    "                  y= ratings_scaled_train, \n",
    "                  epochs= epochs, verbose= 0, batch_size= batch_size, validation_split= 0.1, shuffle= True,\n",
    "                  callbacks=[Learning_Rate_Callback, Save_Progress_Callback()])\n",
    "\n",
    "genres_model.save('models\\\\genres_model.h5', overwrite= True, include_optimizer= True)\n",
    "\n",
    "# _____________________________________________________________________________________________________\n",
    "# Tag Model, Random Forest:\n",
    "rf_input_train = rf_input.iloc[: split_index, :]\n",
    "rf_input_test = rf_input.iloc[split_index :, :]\n",
    "\n",
    "ratings_train = ratings[: split_index]\n",
    "ratings_test = ratings[split_index :]\n",
    "\n",
    "random_forest = RandomForestRegressor(n_estimators= 100, max_features= 'sqrt', verbose=2, random_state= True, n_jobs= -1) ## The number of trees is set to 100 due to high RAM usage\n",
    "random_forest.fit(rf_input_train, ratings_train)\n",
    "print(random_forest.score(rf_input_test, ratings_test))\n",
    "\n",
    "# Saving RF model:\n",
    "pickle.dump(random_forest, open('tags_model.sav', 'wb'))\n",
    "\n",
    "\n",
    "genres_model_predictions = (genres_model.predict(x= [genres_like_test, genres_dislike_test, genres_movie_test])) * 5 # Rescale back to original values\n",
    "random_forest_predict = random_forest.predict(rf_input_test)\n",
    "\n",
    "print('genres Model Stats:')\n",
    "stats(genres_model_predictions, ratings_scaled_test * 5)\n",
    "print('Tags Model Stats:')\n",
    "stats(random_forest_predict, ratings_test)\n",
    "\n",
    "# Creating a input for the combine_model:\n",
    "genres_model_predictions_list = []\n",
    "\n",
    "for prediction in genres_model_predictions:\n",
    "    genres_model_predictions_list.append(prediction[0])\n",
    "    \n",
    "merged_predictions = pd.DataFrame({'genres_model': genres_model_predictions_list, \n",
    "                                   'tag_model': list(random_forest_predict), \n",
    "                                   'genres_true': list(np.array(list(ratings_scaled_test)) * 5), \n",
    "                                   'tag_true': ratings_test}, \n",
    "                                  index= list(range(len(ratings_test))))\n",
    "\n",
    "# Using a linear regression for predictions adjustment:\n",
    "X = merged_predictions.loc[:, ['genres_model', 'tag_model']]\n",
    "y = np.array(merged_predictions.loc[:, 'genres_true']) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25)\n",
    "\n",
    "line_reg = LinearRegression(n_jobs= -1).fit(X_train, y_train)\n",
    "print('Linear Regression R2:', line_reg.score(X_test, y_test))\n",
    "line_reg_predictions = line_reg.predict(X_test)\n",
    "\n",
    "# Saving linear regression model:\n",
    "pickle.dump(line_reg, open('combine_model.sav', 'wb'))\n",
    "\n",
    "# Rounding the predictions that are out of bounds:\n",
    "line_reg_predictions_rounded = []\n",
    "\n",
    "for prediction in line_reg_predictions:\n",
    "    rounded = prediction\n",
    "    if rounded > 5:\n",
    "        rounded = 5\n",
    "    elif rounded < 0.5:\n",
    "        rounded = 0.5\n",
    "    \n",
    "    line_reg_predictions_rounded.append(rounded)\n",
    "        \n",
    "\n",
    "stats(line_reg_predictions_rounded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 19\n",
    "\n",
    "def top_10_recommendations(userId):\n",
    "    # Loading all the datasets needed:\n",
    "    movies_df_mod = pd.read_csv('data\\\\movies_mod.csv', index_col= 0)\n",
    "    ratings_df_removed = pd.read_csv('data\\\\ratings_df_last_liked_movie_removed.csv', index_col= 0)\n",
    "\n",
    "    \n",
    "    # Gathering all the movies in the dataset:\n",
    "    not_watched = list(movies_df_mod.movieId)\n",
    "    \n",
    "    # Selecting all movies that have not been seen by the user:\n",
    "    ratings_df_removed = ratings_df_removed[ratings_df_removed.userId == userId]\n",
    "    \n",
    "    if len(ratings_df_removed) ==  0:  ## First check for valid users/users with enough information \n",
    "        return print('User {} does not have enough information. 1'.format(userId))\n",
    "    \n",
    "    ratings_df_removed = ratings_df_removed.merge(movies_df_mod, how= 'left', on= 'movieId').dropna()\n",
    "    \n",
    "    if len(ratings_df_removed) ==  0:  ## Second check\n",
    "        return print('User {} does not have enough information. 2'.format(userId))\n",
    "    \n",
    "    watched = list(ratings_df_removed.movieId)\n",
    "    del ratings_df_removed  ## I find that not all variables are actually cleared in definitions; this is to ensure it removed from RAM\n",
    "    \n",
    "    # Finding the movies the user has not watched:\n",
    "    for movie in watched:\n",
    "        if movie in not_watched:\n",
    "            not_watched.remove(movie)\n",
    "            \n",
    "    # Loading in users' like and disliked genres:\n",
    "    total_user_like_df = pd.read_csv('data\\\\total_user_like_df.csv', index_col= 0)\n",
    "    total_user_dislike_df = pd.read_csv('data\\\\total_user_dislike_df.csv', index_col= 0) \n",
    "\n",
    "    \n",
    "    # Selecting from total_user_like_df and total_user_dislike_df to isolate only the userId input:\n",
    "    total_user_like_df = total_user_like_df[total_user_like_df.userId == userId]\n",
    "    \n",
    "    if len(total_user_like_df) ==  0:  ## Third check\n",
    "        return print('User {} does not have enough information. 3'.format(userId))\n",
    "    \n",
    "    total_user_dislike_df = total_user_dislike_df[total_user_dislike_df.userId == userId]\n",
    "    if len(total_user_dislike_df) ==  0:  ## Fourth check\n",
    "        return print('User {} does not have enough information. 4'.format(userId))\n",
    "            \n",
    "    # Changing the columns names to differentiate between the columns of total_user_like_df and total_user_dislike_df:\n",
    "\n",
    "    like_columns = list(total_user_like_df.columns)\n",
    "    like_columns_modified = []\n",
    "\n",
    "    for column in like_columns:\n",
    "        if column == 'userId':\n",
    "            like_columns_modified.append('userId')\n",
    "        else:\n",
    "            modify_column = 'user_like_' + column\n",
    "            like_columns_modified.append(modify_column)\n",
    "\n",
    "    total_user_like_df.columns = like_columns_modified\n",
    "    \n",
    "    dislike_columns = list(total_user_dislike_df.columns)\n",
    "    dislike_columns_modified = []\n",
    "\n",
    "    for column in dislike_columns:\n",
    "        if column == 'userId':\n",
    "            dislike_columns_modified.append('userId')\n",
    "        else:\n",
    "            modify_column = 'user_dislike_' + column\n",
    "            dislike_columns_modified.append(modify_column)\n",
    "\n",
    "    total_user_dislike_df.columns = dislike_columns_modified\n",
    "\n",
    "    # Loading in tags:\n",
    "    movie_tags_df = pd.read_csv('data\\\\final\\\\movie_tags_df.csv', index_col= 0)\n",
    "    like_dislike_tags = (pd.read_csv('data\\\\final\\\\like_dislike_tags.csv', index_col= 0)).astype('int64')\n",
    "    \n",
    "    # Selecting the movies that have not been seen from movie_tags_df and merging movies_df_mod and movie_tags_df:\n",
    "    template_df = pd.DataFrame({'movieId': not_watched}, index= list(range(len(not_watched)))) ## Creating a template DF for merging\n",
    "    template_df = template_df.merge(movies_df_mod, how= 'left', on= 'movieId').dropna()\n",
    "    template_df = template_df.merge(movie_tags_df, how= 'left', on= 'movieId').dropna()\n",
    "    del movie_tags_df\n",
    "    \n",
    "    # Selecting the user's tags:\n",
    "    like_dislike_tags = like_dislike_tags[like_dislike_tags.userId == userId]\n",
    "    if len(like_dislike_tags) ==  0:  ## Fifth check\n",
    "        return print('User {} does not have enough information. 5'.format(userId))\n",
    "\n",
    "    # Adding a userId column to the template DF so that merging is possible with total_user_like_df, total_user_dislike_df, and like_dislike_tags\n",
    "    template_df['userId'] = userId\n",
    "    template_df = template_df.merge(total_user_like_df, how= 'left', on= 'userId').dropna()\n",
    "    del total_user_like_df\n",
    "    template_df = template_df.merge(total_user_dislike_df, how= 'left', on= 'userId').dropna()\n",
    "    del total_user_dislike_df\n",
    "    template_df = template_df.merge(like_dislike_tags, how= 'left', on= 'userId').dropna()\n",
    "    del like_dislike_tags\n",
    "    \n",
    "    like_columns_modified.remove('userId')\n",
    "    dislike_columns_modified.remove('userId')\n",
    "    like_columns.remove('userId')\n",
    "\n",
    "    # Generating the columns for the tag inputs for random forest:\n",
    "    rf_columns = []\n",
    "    for x in range(20): \n",
    "        rf_columns.append('LIKE_' + str(x))\n",
    "        rf_columns.append('DISLIKE_' + str(x))\n",
    "    for x in range(5):\n",
    "        rf_columns.append('TAG_' + str(x))\n",
    "        \n",
    "    # Selecting out the inputs from the template DF by column names:\n",
    "    genres_like_input = template_df.loc[:, like_columns_modified]\n",
    "    genres_dislike_input = template_df.loc[:, dislike_columns_modified]\n",
    "    genres_movie_input = template_df.loc[:, like_columns]\n",
    "    \n",
    "    tags_input = template_df.loc[:, rf_columns]\n",
    "    \n",
    "    # Saving the movieId list:\n",
    "    movieId_list = list(template_df.movieId)\n",
    "    \n",
    "    del template_df\n",
    "    \n",
    "    # Loading in all models\n",
    "    genres_model = tf.keras.models.load_model('models\\\\genres_model.h5', compile=True)\n",
    "    tags_model = pickle.load(open('tags_model.sav', 'rb'))\n",
    "    combine_model = pickle.load(open('combine_model.sav', 'rb'))\n",
    "    \n",
    "    # Predicting with the genres model and tags model:\n",
    "    genres_model_predictions = (genres_model.predict(x= [genres_like_input, genres_dislike_input, genres_movie_input])) * 5 ## Rescaling up; predicts a scaled and bound (sigmoid, 0-1) values\n",
    "    tags_model_predictions = tags_model.predict(tags_input)\n",
    "    \n",
    "    # Extracting and changing the Keras predictions into a 1-D format (list):\n",
    "    genres_model_predictions_list = []\n",
    "\n",
    "    for prediction in genres_model_predictions:\n",
    "        genres_model_predictions_list.append(prediction[0])\n",
    "    \n",
    "    # Using the predictions from the two models as the inputs for the combine_model:\n",
    "    combine_input = pd.DataFrame({'genres_predictions': genres_model_predictions_list, \n",
    "                                  'tags_predictions': tags_model_predictions}, \n",
    "                                 index= list(range(len(genres_model_predictions))))\n",
    "    \n",
    "    combine_model_predictions = combine_model.predict(combine_input)\n",
    "    \n",
    "    # Rounding the predictions that are out of bounds:\n",
    "    combine_model_predictions_rounded = []\n",
    "\n",
    "    for prediction in combine_model_predictions:\n",
    "        rounded = prediction\n",
    "        if rounded > 5:\n",
    "            rounded = 5\n",
    "        elif rounded < 0.5:\n",
    "            rounded = 0.5\n",
    "\n",
    "        combine_model_predictions_rounded.append(rounded)\n",
    "    \n",
    "    # Adding all predictions into one DF:\n",
    "    predictions_df = pd.DataFrame({'movieId': movieId_list,\n",
    "                                   'genres_predictions': genres_model_predictions_list, \n",
    "                                  'tags_predictions': tags_model_predictions,\n",
    "                                  'combine_predictions': combine_model_predictions_rounded}, \n",
    "                                 index= list(range(len(movieId_list))))\n",
    "    \n",
    "    # Sorting by combine_model_predictions_rounded and selecting the first 10 highest predicted ratings:\n",
    "    best_movies_df = predictions_df.sort_values(by= ['combine_predictions'], ascending=False).iloc[:10, :]\n",
    "    \n",
    "    # Finding adding the movie titles and information to highest 10:\n",
    "    best_movies_df = best_movies_df.merge(movies_df_mod, how= 'left', on= 'movieId').dropna()\n",
    "    del movies_df_mod\n",
    "    \n",
    "    return predictions_df, best_movies_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 20\n",
    "\n",
    "predictions_df, best_movies_df = top_10_recommendations(6550) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 21\n",
    "\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 22\n",
    "\n",
    "best_movies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional:\n",
    "If wanting to loop through all users and save the predictions, remove the triple single quotes at the start and end of the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 23\n",
    "\n",
    "'''\n",
    "if os.path.exists('predictions\\\\') != True: \n",
    "    os.mkdir('predictions\\\\')\n",
    "\n",
    "if os.path.exists('predictions\\\\full_predictions') != True: \n",
    "    os.mkdir('predictions\\\\full_predictions')\n",
    "    \n",
    "if os.path.exists('predictions\\\\top_10') != True: \n",
    "    os.mkdir('predictions\\\\top_10')\n",
    "\n",
    "ratings_df_removed = pd.read_csv('data\\\\ratings_df_last_liked_movie_removed.csv', index_col= 0)\n",
    "userId_list = list(set(ratings_df_removed.userId))\n",
    "del ratings_df_removed\n",
    "\n",
    "progress_counter_1 = 0\n",
    "progress_counter_2 = 5\n",
    "\n",
    "for user in userId_list:\n",
    "    progress_counter_1 += 1\n",
    "    \n",
    "    predictions_df, best_movies_df = top_10_recommendations(user) \n",
    "    \n",
    "    predictions_df.to_csv('predictions\\\\full_predictions\\\\full_predictions - {}.csv'.format(user))\n",
    "    \n",
    "    best_movies_df.to_csv('predictions\\\\top_10\\\\top_10 - {}.csv'.format(user))\n",
    "    \n",
    "    if progress_counter_1 / len(userId_list) * 100 >= progress_counter_2:\n",
    "        print(progress_counter_1 / len(userId_list) * 100, '% Completed')\n",
    "        progress_counter_2 += 5\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
